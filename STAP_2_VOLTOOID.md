# ‚úÖ Stap 2: Ollama Setup - VOLTOOID!

## ‚úÖ Alles Werkt!

- [x] ‚úÖ Ollama ge√Ønstalleerd (versie 0.14.2)
- [x] ‚úÖ Ollama draait op http://localhost:11434
- [x] ‚úÖ Model `mistral:7b` gedownload (4.4 GB)
- [x] ‚úÖ Backend geconfigureerd voor Ollama
- [x] ‚úÖ Local LLM service werkt
- [x] ‚úÖ Health check uitgebreid met LLM status

## üß™ Test Resultaten

### Ollama Direct Test
```powershell
$body = @{model="mistral:7b"; prompt="Hallo, kun je dit lezen?"; stream=$false} | ConvertTo-Json
Invoke-RestMethod -Uri "http://localhost:11434/api/generate" -Method Post -Body $body -ContentType "application/json"
```
**Resultaat**: ‚úÖ "Ja, ik kan het Nederlands lezen! Hoe kan ik je helpen?"

### Backend Health Check
```powershell
curl http://localhost:8000/health
```
**Resultaat**: 
```json
{
  "status": "healthy",
  "version": "0.1.0",
  "local_llm": {
    "backend": "ollama",
    "model": "mistral:7b",
    "available": true
  }
}
```

### Local LLM Service Test
```powershell
cd backend
.\venv\Scripts\python.exe -c "from app.services.local_llm import get_local_llm_service; llm = get_local_llm_service(); print(f'Backend: {llm.backend}'); print(f'Model: {llm.model_name}'); print(f'Health: {llm.health_check()}')"
```
**Resultaat**: 
- Backend: ollama
- Model: mistral:7b
- Health: True

## üìã Configuratie

### Backend Config (`backend/app/core/config.py`)
```python
LOCAL_LLM_BACKEND: str = "ollama"
OLLAMA_BASE_URL: str = "http://localhost:11434"
OLLAMA_MODEL: str = "mistral:7b"
```

### Environment Variables (`.env`)
Deze worden automatisch geladen vanuit `config.py`. Je kunt ze overschrijven in `.env`:
```env
LOCAL_LLM_BACKEND=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral:7b
```

## üéØ Wat Werkt Nu

1. **Lokale AI**: Alle AI queries draaien nu lokaal via Ollama
2. **Privacy**: Geen data gaat naar externe API's
3. **Nederlands**: Model ondersteunt Nederlands
4. **Health Monitoring**: Backend health check toont LLM status

## üìù Gebruik

### Chat API Testen
```powershell
# Eerst inloggen (auth token nodig)
$token = "your-auth-token"
$headers = @{Authorization="Bearer $token"}
$body = @{message="Wat zijn goede coaching technieken?"} | ConvertTo-Json
Invoke-RestMethod -Uri "http://localhost:8000/api/chat" -Method Post -Body $body -Headers $headers -ContentType "application/json"
```

### Direct LLM Test (via Python)
```python
from app.services.local_llm import get_local_llm_service

llm = get_local_llm_service()
response = llm.generate(
    prompt="Geef een kort overzicht van coaching technieken.",
    max_tokens=200,
    temperature=0.7
)
print(response)
```

## ‚úÖ Volgende Stappen

### Stap 3: Frontend Setup
1. Frontend dependencies installeren
2. Frontend configureren voor backend API
3. Chat interface testen

### Stap 4: RAG Setup (Optioneel)
1. Coaching materialen toevoegen
2. Vector database configureren
3. Embeddings testen

## üéâ Stap 2 Klaar!

Ollama is volledig geconfigureerd en werkt! De backend gebruikt nu lokale AI voor alle queries. üöÄ
